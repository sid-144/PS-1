# -*- coding: utf-8 -*-
"""Audio_Processing_Working_model_with_summary.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-WvpDm2xEEg9gz32hx8ekdx--6axdOmn
"""

!pip install -q praat-parselmouth
!pip install -q pyannote.core
!pip install -q scikit-learn
!pip install -q librosa > /dev/null
!pip install -q transformers > /dev/null
!pip install -q jiwer > /dev/null
!pip install -q torchaudio > /dev/null
!pip install pyannote.audio
!pip install openai-whisper

!pip install openai-whisper

import librosa
import whisper
import datetime
import subprocess
import torch
from pyannote.core import Segment
import wave
import contextlib
from google.colab import files
import numpy as np
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import euclidean_distances  # Replace cosine_similarity
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import parselmouth

# Function to format time
def time(secs):
    return datetime.timedelta(seconds=round(secs))

# Upload the audio file for transcription
uploaded = files.upload()
path = next(iter(uploaded))

# Upload sample audio clips for each speaker
print("Upload sample audio clips for each speaker")
sample_audios = {}
num_speakers = int(input("Enter the number of speakers: "))
for i in range(num_speakers):
    print(f"Upload sample audio clip for speaker {i + 1}")
    sample_upload = files.upload()
    sample_path = next(iter(sample_upload))
    sample_label = input(f"Enter label for speaker {i + 1}: ")
    sample_audios[sample_label] = sample_path

# Set the language and model size
language = 'English'
model_size = 'large'

# Adjust model name based on language and size
model_name = model_size
if language == 'English' and model_size != 'large':
    model_name += '.en'

# Load Whisper model
model = whisper.load_model(model_size)

# Convert audio to wav format if needed
if path[-3:] != 'wav':
    subprocess.call(['ffmpeg', '-i', path, 'audio.wav', '-y'])
    path = 'audio.wav'

# Convert sample audios to wav format if needed
for label, sample_path in sample_audios.items():
    if sample_path[-3:] != 'wav':
        subprocess.call(['ffmpeg', '-i', sample_path, f'{label}.wav', '-y'])
        sample_audios[label] = f'{label}.wav'

# Transcribe audio
result = model.transcribe(path)
segments = result["segments"]

print(result)

with contextlib.closing(wave.open(path, 'r')) as f:
    frames = f.getnframes()
    rate = f.getframerate()
    duration = frames / float(rate)

def extract_features(waveform, sr):
    sound = parselmouth.Sound(waveform, sampling_frequency=sr)
    formant = sound.to_formant_burg()

    # Get formant frequencies for the middle of the audio
    formants = [formant.get_value_at_time(i, sound.get_total_duration()/2) for i in range(1, 5)]  # Extract up to 4 formants
    formants = np.array(formants)

    # MFCCs
    mfccs = librosa.feature.mfcc(y=waveform, sr=sr, n_mfcc=13).mean(axis=1)

    # Spectral Contrast
    spectral_contrast = librosa.feature.spectral_contrast(y=waveform, sr=sr).mean(axis=1)

    # Chroma STFT
    chroma_stft = librosa.feature.chroma_stft(y=waveform, sr=sr).mean(axis=1)

    # Pitch
    pitch = parselmouth.praat.call(sound, "To Pitch", 0.0, 75, 600)
    pitch_values = pitch.selected_array['frequency'].flatten()
    pitch_mean = np.mean(pitch_values) if pitch_values.size > 0 else 0

    # Harmonic-to-Noise Ratio
    hnr = parselmouth.praat.call(sound, "To Harmonicity (cc)", 0.01, 75, 600, 1.0)
    hnr_values = hnr.values.flatten()
    hnr_mean = np.mean(hnr_values) if hnr_values.size > 0 else 0

    # Jitter
    point_process = parselmouth.praat.call(sound, "To PointProcess (periodic, cc)", 75, 600)
    jitter = parselmouth.praat.call(point_process, "Get jitter (local)", 0.0, 0.02, 0.001, 0.03, 1.3)
    jitter_value = jitter if jitter is not None else 0

    # Shimmer
    shimmer = parselmouth.praat.call([sound, point_process], "Get shimmer (local)", 0.0, 0.02, 0.001, 0.03, 1.6, 1.0)
    shimmer_value = shimmer if shimmer is not None else 0

    features = np.hstack((
        formants,
        mfccs,
        spectral_contrast,
        chroma_stft,
        pitch_mean,
        hnr_mean,
        jitter_value,
        shimmer_value
    ))

    return features

# Extract features for sample audios
sample_features = {label: extract_features(*librosa.load(sample_path, sr=16000)) for label, sample_path in sample_audios.items()}

# Extract features for audio segments
segment_features = []
for segment in segments:
    start_time = segment["start"]
    end_time = segment["end"]
    segment_waveform, sr = librosa.load(path, sr=16000, offset=start_time, duration=end_time - start_time)
    segment_features.append(extract_features(segment_waveform, sr))

# Combine and standardize the features
combined_features = np.vstack((segment_features, list(sample_features.values())))
scaler = StandardScaler()
combined_features = scaler.fit_transform(combined_features)

# Split back the standardized features
segment_features = combined_features[:len(segment_features)]
sample_features = combined_features[len(segment_features):]

# Impute NaN values with the mean of the column
imputer = SimpleImputer(strategy='mean')
segment_features_imputed = imputer.fit_transform(segment_features)
sample_features_imputed = imputer.fit_transform(sample_features)

# Apply PCA to reduce dimensions
pca = PCA(n_components=2)
reduced_features = pca.fit_transform(np.vstack((segment_features_imputed, sample_features_imputed)))
reduced_segment_features = reduced_features[:len(segment_features)]
reduced_sample_features = reduced_features[len(segment_features):]

# Calculate distances and assign speaker labels based on the minimum distance in PCA space
speaker_mapping = {label: [] for label in sample_audios.keys()}
for i, segment_feature in enumerate(reduced_segment_features):
    distances = {
        sample_label: euclidean_distances(
            segment_feature.reshape(1, -1),
            reduced_sample_features[j].reshape(1, -1)
        )[0][0]
        for j, sample_label in enumerate(sample_audios.keys())
    }
    assigned_label = min(distances, key=distances.get)  # Minimize Euclidean distance
    speaker_mapping[assigned_label].append(i)

# Save transcript with speaker labels
transcript_path = "transcript.txt"
with open(transcript_path, "w") as f:
    for i, segment in enumerate(segments):
        current_segment = Segment(segment["start"], segment["end"])
        current_speaker_label = None
        for diarized_segment, speakers in speaker_mapping.items():
            if i in speakers:
                current_speaker_label = diarized_segment
                break

        if current_speaker_label is None:
            current_speaker_label = "Unknown"

        if i == 0 or segments[i - 1].get("speaker") != current_speaker_label:
            f.write("\n" + current_speaker_label + ' ' + str(time(segment["start"])) + '\n')
        f.write(segment["text"][1:] + ' ')

print("Transcript with speaker labels saved to transcript.txt")

# Print the transcript
with open(transcript_path, "r") as f:
    transcript_content = f.read()

print("\nTranscript:\n")
print(transcript_content)

# Visualize the features using PCA
plt.figure(figsize=(10, 6))
for i, (x, y) in enumerate(reduced_segment_features):
    plt.scatter(x, y, label=f'Segment {i+1}')
    plt.text(x, y, f'{i+1}', fontsize=9, ha='right')
for i, (sample_label, reduced_sample_feature) in enumerate(zip(sample_audios.keys(), reduced_sample_features)):
    plt.scatter(reduced_sample_feature[0], reduced_sample_feature[1], label=sample_label, marker='x', s=200)
plt.legend()
plt.title('PCA of Segment and Sample Features')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

GROQ_API_KEY=userdata.get('groqapikey')

!pip install groq

import os

from groq import Groq

client = Groq(
    api_key=GROQ_API_KEY,
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "summarize" + transcript_content,
        }
    ],
    model="llama3-70b-8192",
)

print(chat_completion.choices[0].message.content)
summary_path = "summary.txt"
with open(summary_path, "w") as f:
    f.write(chat_completion.choices[0].message.content)

print("\nSummary:\n")
print(chat_completion.choices[0].message.content)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "summarize what each person said" + transcript_content,
        }
    ],
    model="llama3-70b-8192",
)

print(chat_completion.choices[0].message.content)
